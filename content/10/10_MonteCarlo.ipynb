{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "saving-objective",
   "metadata": {},
   "source": [
    "The Canonical Ensemble\n",
    "======================\n",
    "\n",
    "Up to this point, we have primarily considered constant energy systems (microcanonical ensemble). However, most physical systems are not isolated, but exchange energy with the\n",
    "environment. Since the systems are very small compared to the environment,\n",
    "we consider that the environment acts effectively as a heat reservoir or\n",
    "heat bath at a fixed temperature $T$. If a small system is put in\n",
    "thermal contact with the heat bath, it will reach thermal equilibrium\n",
    "exchanging energy until the properties of the system reflect the temperature of the bath.\n",
    "\n",
    "Imagine an infinitely large number of mental copies of the system and\n",
    "the heat bath. The probability $P_s$ that the system is found in a\n",
    "microstate $s$ with energy $s$ is given by:\n",
    "$$P_s=\\frac{1}{Z}e^{-E_s/k_BT},\n",
    "$$ where $Z$ is the normalization constant. This\n",
    "corresponds to the canonical ensemble. Since $\\sum P_s = 1$, we have\n",
    "$$Z=\\sum_s{e^{-E_s/k_BT}},\n",
    "$$ where the sum is over all the possible microstates of the\n",
    "system. This equation defines the “partition function” of the system.\n",
    "\n",
    "We can use this relation to obtain the ensemble average of physical\n",
    "quantities of interest. For instance, the mean energy is given by:\n",
    "$$\\langle E \\rangle = \\sum_s E_s\\, P_s=\\frac{1}{Z}\\sum_s{E_s\\,e^{-\\beta\n",
    "E_s}},$$ with $\\beta=1/k_BT$.\n",
    "\n",
    "The Metropolis algorithm\n",
    "------------------------\n",
    "\n",
    "We want to obtain an estimate for the mean value of an observable $A$:\n",
    "$$\\langle A \\rangle = \\sum_s A_s e^{-\\beta E_s}/\\sum_s e^{-\\beta E_s},$$\n",
    "where $E_s$ and $A_s$ are the values of the energy and the quantity $A$\n",
    "in the configuration $s$. The idea of using Monte Carlo consists in\n",
    "sampling a subset of configurations, where the probability of each configuration follows a Boltzmann distribution. The ensemble average is then estimated  by:\n",
    "$$\\langle A \\rangle \\simeq \\sum_s^{m} A_s e^{-\\beta E_s}/\\sum_s^{m}\n",
    "e^{-\\beta E_s},$$ where the sum is over $m$ configurations. That is, rather than evaluating the energy for every possible microstate, we only want to consider those that are \"likely\" to be reached at the given temperature. \n",
    "\n",
    "A crude Monte Carlo procedure is to generate a configuration at random,\n",
    "calculate $E_s$ and $A_s$, and add the contributions of this configuration\n",
    "to the sum. This is equivalent to the “hit and miss” Monte Carlo method\n",
    "for evaluating integrals. However, this would be very inefficient, because the configurations generated would likely be very\n",
    "improbable and contribute very little to the sum. Instead, we want to\n",
    "generate a sample of configurations that are <span>*important*</span>,\n",
    "<span>*i. e.*</span> have large contributions to the sum. This is equivalent to “importance sampling”. Hence, we need to\n",
    "generate the configurations according to a probability distribution. In\n",
    "this case, the most convenient is the Boltzmann\n",
    "probability itself $P_s$. Since we will average over the\n",
    "$m$ configurations generated with this probability, we must use the\n",
    "expression:\n",
    "$$\\langle A \\rangle \\simeq \\sum_s^{m} \\frac{A_s}{P_s} e^{-\\beta\n",
    "E_s}/\\sum_s^{m} \\frac{1}{P_s}e^{-\\beta E_s}\n",
    "= \\frac{1}{n}\\sum_s A_i$$\n",
    "where the sum over $i$ is the sum over the $n$ sampled configurations in the MC simulation. \n",
    "\n",
    "\n",
    "The idea of the Monte Carlo algorithm is to attempt a random\n",
    "walk over the space of configurations. The walker “hops” from a\n",
    "configuration $i$ to another $j$ using an appropriate “transition probability”  $W$.  Specifically, we define equilibrium as the ensemble over which there is no net flux between states. That is, the frequency of interconversion between states is equal in opposite directions.  This may be described as obeying the\n",
    "“detailed balance” condition:\n",
    "$$W(i \\rightarrow j)e^{-\\beta E_i} = W(j \\rightarrow i)e^{-\\beta E_j}$$\n",
    "\n",
    "In a MC simulation, we require detailed balance to be satisfied. To do this, we reexpress the relation as:\n",
    "\n",
    "$$\\frac{W(i \\rightarrow j)}{W(j \\rightarrow i)} = \\frac{e^{-\\beta E_j}}{e^{-\\beta E_i}}=e^{-\\beta (E_j-E_i)}$$\n",
    "\n",
    "\n",
    "We can then define the probability of transitioning from state $i$ to $j$ as :\n",
    "\n",
    "$$W=\\min{\\left(1,e^{-\\beta(E_j-E_i)}\\right)}.$$\n",
    "\n",
    "That is, if the energy decreases when the system is moved, accept the move. If the move increases the energy, use the exponential relation to calculate the probability of accepting the move.\n",
    "\n",
    "\n",
    "Note: Here we are defining $\\beta$ as $1/k_BT$.  However, one could consider more general problems where there is a scoring function that must be minimized $S$.  In that case, moves are attempted, and the probability of accepting the move is given by:\n",
    "\n",
    "$$W=\\min{\\left(1,e^{-(S_j-S_i)/T_{eff}}\\right)}$$\n",
    "\n",
    "In these cases, $T_{eff}$ is the effective temperature, in that is controls the moves in a manner than is consistent  with the role of temperature in an energetic problem.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The pseudocode for a Monte Carlo simulation can be outlined as follows:\n",
    "\n",
    "1.  Establish an initial configuration.\n",
    "\n",
    "2.  Make a random trial change in the configuration. For example, choose\n",
    "    a spin at random and try to flip it. Or choose a particle at random\n",
    "    and attempt to displace it a random distance.\n",
    "\n",
    "3.  Compute the change in the energy of the system $\\Delta E$ due to the\n",
    "    trial change.\n",
    "\n",
    "4.  If $\\Delta E \\leq 0$, accept the new configuration and go to step 8.\n",
    "\n",
    "5.  If $\\Delta E$ is positive, compute the “transition probability”\n",
    "    $W=e^{-\\beta \\Delta E}$.\n",
    "\n",
    "6.  Generate a random number $r$ in the interval $[0,1]$.\n",
    "\n",
    "7.  If $r \\leq W$, accept the new configuration; otherwise retain the\n",
    "    previous configuration.\n",
    "\n",
    "8.  Repeat steps (2) to (7) to obtain a sufficient number of\n",
    "    configurations or “trials”.\n",
    "\n",
    "9.  Compute averages over configurations which are statistically\n",
    "    independent of each other.\n",
    "    \n",
    "### Important conditions for validity\n",
    "\n",
    "A Monte Carlo algorithm must satisfy detailed balance, but also **Ergodicity**. This means that the possible moves should guarantee that the system will explore the entire phase space. If there are regions of phase space that are not accessible via local moves, for instance, one should implement global moves or more complex update strategies.\n",
    "\n",
    "## HW 10\n",
    "\n",
    "Problem 1: Write a program that will simulate a 1D ising system.  That is, each spin interacts with its nearest neighbor, with a spin-spin coupling constant (for convenience, use 1).  Employ periodic boundary conditions, so that spin 1 and N are coupled. Using reduced units, calculate the specific heat at T=0.2$*$NT, for $1\\le NT \\lt \\ 25$. Study a system with 100 spins. To equilibrate the system, perform $10^6$ MC trials.  Then, for production, perform $10^7$ trials.  If there is a (pseudo) phase transition between an ordered and disordered state, then there should be a peak in the specific heat.  Is there a phase transition?  Is this consistent with expectations? Tip: It is probably easiest to calculate the specific heat by using the variance of the energies (think back to Phys 7305).\n",
    "\n",
    "Problem 2: Repeat problem 1 using a 2D ising system. Again using 100 spins, but in a square lattice. If your code is efficient, you can crank the number of spins up to 10000 and try to run it on the cluster.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-treat",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
